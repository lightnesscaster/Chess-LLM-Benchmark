# Chess LLM Benchmark Configuration

# Benchmark settings
benchmark:
  games_vs_anchor_per_color: 3    # ~5-6 games per LLM vs anchor (3 as each color)
  games_vs_llm_per_color: 2       
  max_moves: 200                   # Maximum half-moves per game
  max_concurrent: 10               # Maximum concurrent games
  rating_threshold: 600            # Only pair players within this rating difference

# Engine anchors (fixed ratings)
engines:
  - player_id: "random-bot"
    type: random
    rating: 400

  - player_id: "maia-1100"
    type: maia
    lc0_path: "/opt/homebrew/bin/lc0"
    weights_path: "/Volumes/MainStorage/Programming/create_chess_puzzles/maia-1100.pb"
    rating: 1628
    nodes: 1

  - player_id: "maia-1900"
    type: maia
    lc0_path: "/opt/homebrew/bin/lc0"
    weights_path: "/Volumes/MainStorage/Programming/chess_llm_benchmark/maia-1900.pb.gz"
    rating: 1816
    nodes: 1

  - player_id: "eubos"
    type: uci
    path: "/Volumes/MainStorage/Programming/EubosChess/eubos.sh"
    rating: 2211
    initial_time: 900
    increment: 10



# LLM players - budget models only
llms:
  - player_id: "gpt-5.1-chat"
    model_name: "openai/gpt-5.1-chat"
    temperature: 0.0
  # gpt-5-chat (228)
  - player_id: "gpt-5-chat"
    model_name: "openai/gpt-5-chat"
    temperature: 0.0
    
  - player_id: "gpt-5.2-chat"
    model_name: "openai/gpt-5.2-chat"
    temperature: 0.0

  - player_id: "gpt-5 (no thinking)"
    model_name: "openai/gpt-5"
    temperature: 0.0

  - player_id: "gpt-5.1 (no thinking)"
    model_name: "openai/gpt-5.1"
    temperature: 0.0

  - player_id: "gpt-5.2 (no thinking)"
    model_name: "openai/gpt-5.2"
    temperature: 0.0

  # gemini-2.5-flash (no thinking) (223)
  - player_id: "gemini-2.5-flash (no thinking)"
    model_name: "google/gemini-2.5-flash"
    temperature: 0.0

  # gemini-2.0-flash-001 (180)
  - player_id: "gemini-2.0-flash-001"
    model_name: "google/gemini-2.0-flash-001"
    temperature: 0.0

  # glm-4.6 (no thinking) (-62)
  - player_id: "glm-4.6 (no thinking)"
    model_name: "z-ai/glm-4.6"
    temperature: 0.0

  # deepseek-v3.2-exp (no thinking) (-107)
  - player_id: "deepseek-v3.2 (no thinking)"
    model_name: "deepseek/deepseek-v3.2"
    temperature: 0.0
    timeout: 1200

  # deepseek-v3.2 (no thinking) (-107)
  - player_id: "deepseek-v3.2 (no thinking)"
    model_name: "deepseek/deepseek-v3.2-exp"
    temperature: 0.0
    timeout: 1200

  # kimi-k2-0905 (-240)
  - player_id: "kimi-k2-0905"
    model_name: "moonshotai/kimi-k2-0905"
    temperature: 0.0
    timeout: 1200

  # deepseek-v3.1-terminus (no thinking) (-350)
  - player_id: "deepseek-v3.1-terminus (no thinking)"
    model_name: "deepseek/deepseek-v3.1-terminus"
    temperature: 0.0
    timeout: 1200

  # kimi-k2 (-388)
  - player_id: "kimi-k2"
    model_name: "moonshotai/kimi-k2"
    temperature: 0.0
    timeout: 1200

  # qwen3-235b-a22b-2507 (-453)
  - player_id: "qwen3-235b-a22b-2507"
    model_name: "qwen/qwen3-235b-a22b-2507"
    temperature: 0.0

  # deepseek-chat-v3-0324 (-453)
  - player_id: "deepseek-chat-v3-0324"
    model_name: "deepseek/deepseek-chat-v3-0324"
    temperature: 0.0
    timeout: 1200

  # mistral-medium-3 (-453)
  - player_id: "mistral-medium-3"
    model_name: "mistralai/mistral-medium-3"
    temperature: 0.0

  # gpt-3.5-turbo-0613 (-453)
  - player_id: "gpt-3.5-turbo-0613"
    model_name: "openai/gpt-3.5-turbo-0613"
    temperature: 0.0

  # deepseek-chat-v3.1 (no thinking) (-453)
  - player_id: "deepseek-chat-v3.1 (no thinking)"
    model_name: "deepseek/deepseek-chat-v3.1"
    temperature: 0.0
    timeout: 1200

  # gpt-5-mini (minimal) (-453)
  - player_id: "gpt-5-mini (minimal)"
    model_name: "openai/gpt-5-mini"
    temperature: 0.0
    reasoning_effort: minimal

  # llama-3.3-70b-instruct (-453)
  - player_id: "llama-3.3-70b-instruct"
    model_name: "meta-llama/llama-3.3-70b-instruct"
    temperature: 0.0

  # gpt-3.5-turbo (-624)
  - player_id: "gpt-3.5-turbo"
    model_name: "openai/gpt-3.5-turbo"
    temperature: 0.0

  # llama-4-maverick (-704)
  - player_id: "llama-4-maverick"
    model_name: "meta-llama/llama-4-maverick"
    temperature: 0.0

  # --- Reasoning models ---

  - player_id: "deepseek-r1-distill-qwen-32b"
    model_name: "deepseek/deepseek-r1-distill-qwen-32b"
    temperature: 0.0
    reasoning: true
    timeout: 1200

  - player_id: "grok-3-mini"
    model_name: "x-ai/grok-3-mini"
    temperature: 0.0
    reasoning: true

  - player_id: "glm-4.6 (thinking)"
    model_name: "z-ai/glm-4.6"
    temperature: 0.0
    reasoning: true

  - player_id: "gpt-oss-120b (high)"
    model_name: "openai/gpt-oss-120b"
    temperature: 0.0
    reasoning_effort: high
    provider_order: ["DeepInfra"]

  - player_id: "gpt-oss-120b (medium)"
    model_name: "openai/gpt-oss-120b"
    temperature: 0.0
    reasoning_effort: medium
    provider_order: ["DeepInfra"]

  # deepseek-v3.2-exp (thinking)
  - player_id: "deepseek-v3.2-exp (thinking)"
    model_name: "deepseek/deepseek-v3.2-exp"
    temperature: 0.0
    timeout: 1200

  # # kimi-k2-thinking - is too slow/unreliable to run
  # - player_id: "kimi-k2-thinking"
  #   model_name: "moonshotai/kimi-k2-thinking"
  #   temperature: 0.0
  #   timeout: 1200

  # gpt-5.2 (high)
  - player_id: "gpt-5.2 (high)"
    model_name: "openai/gpt-5.2"
    temperature: 0.0
    reasoning_effort: high

  # deepseek-v3.2 (thinking) - is too slow/unreliable to run
  # - player_id: "deepseek-v3.2 (thinking)"
  #   model_name: "deepseek/deepseek-v3.2"
  #   temperature: 0.0
  #   reasoning: true
  #   timeout: 1200
  #   provider_order: ["Novita"]

  # gemini-3-pro-preview
  - player_id: "gemini-3-pro-preview"
    model_name: "google/gemini-3-pro-preview"
    temperature: 0.0

  # grok-4.1-fast
  - player_id: "grok-4.1-fast"
    model_name: "x-ai/grok-4.1-fast"
    temperature: 0.0

  # gpt-5.1 (high)
  - player_id: "gpt-5.1 (high)"
    model_name: "openai/gpt-5.1"
    temperature: 0.0
    reasoning_effort: high